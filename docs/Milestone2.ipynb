{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 2\n",
    "\n",
    "**Due: Thursday, Nov 8th at 11:59 PM**\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Automatic differentiation (AD) is implemented in this software. More specifically, it can automatically differentiate of a python function up to machine precision and it can take derivatives of derivatives. Obtaining derivatives accurately is important because it is the key part of gradient-based optimization, which is the foundation of many machine learning algorithms. As a matter of fact, most machine learning problems can be divided into following steps:\n",
    "\n",
    "1. define a function connecting some input $X$ with some output $Y$ with a set of parameters $\\beta$ as $Y = f(X,\\beta)$;\n",
    "2. define a loss function to check how good the model is $L(X,Y,\\beta)$;\n",
    "3. find the parameter set $\\beta$ that minimize the loss function: argmin$_\\beta L(X,Y,\\beta)$.\n",
    "\n",
    "Generally the power or performance of a machine learning algorithm is limited by the third step which is can be handled by gradient-based optimization. Therefore, our software can be used in various machine learning packages and boost their performances.\n",
    "\n",
    "Furthermore, AD can also be applied to solve differential equations in various physical systems. Such as, diffusion equations, wave equations, Navierâ€“Stokes equations and other non-linear equations which cannot be solved analytically. Traditional numerical method using difference method possess error much larger than machine error. Therefore, applying AD will possibly increase the accuracy of the solvers of those differential equations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use *AutoDiff*\n",
    "### Installation\n",
    "\n",
    "### Introduction to basic usage of the package\n",
    "\n",
    "After successful installation, the user will first import our package.\n",
    "```python\n",
    "import autodiff as ad\n",
    "```\n",
    "Then depending on the type of expressions they have, they will employ one of the following methods.\n",
    "\n",
    "#### Scalar functions of scalar values\n",
    "Say the user wants to get the gradient of the expression $f(x) = alpha * x + 3$.\n",
    "The user will first create a variable x and then define the symbolic expression for `f`.\n",
    "```python\n",
    "a = 2.0\n",
    "x = ad.Variable(a, name='x')\n",
    "f = 2 * x + 3\n",
    "```\n",
    "Note: If the user wants to include special functions like sin and exp, they need to do the following:\n",
    "```python\n",
    "f = 2 * ad.Sin(x) + 3\n",
    "```\n",
    "Then when they want to evaluate the gradients of f with respect to x, they will do\n",
    "```python\n",
    "print(f.val, f.der)\n",
    "```\n",
    "f.val and f.der will then contain the value and gradient of f with respect to x.\n",
    "\n",
    "#### Scalar functions of vectors\n",
    "Say the user wants to get the gradient of the expression $f(x1,x2) = x_1 x_2 + x_1$. \n",
    "\n",
    "The user will first create two variables `x1` and `x2` and then define the symbolic expression for `f`.\n",
    "```python\n",
    "a1 = 2.0\n",
    "a2 = 3.0\n",
    "x1 = ad.Variable(a1,name='x1')\n",
    "x2 = ad.Variable(a2,name='x2')\n",
    "f = x1 * x2 + x_1\n",
    "```\n",
    "Then when they want to get the values and gradients of f with respect to x1 and x2, they will do\n",
    "```python\n",
    "print(f.val, f.der)\n",
    "```\n",
    "f.val and f.der will then contain dictionaries of values and gradients of f with respect to x1 and x2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector functions of vectors\n",
    "Say the user wants to get the gradients of the system of functions \n",
    "$$f_1 = x_1 x_2 + x_1$$\n",
    "$$f_2 = \\frac{x_1}{x_2}$$\n",
    "\n",
    "i.e.\n",
    "$$\\mathbf{f}(x1,x2)=(f_1(x_1,x_2),f_2(x_1,x_2))$$\n",
    "The user will first create two variables `x1` and `x2` and then define the symbolic expression for `f`.\n",
    "```python\n",
    "x1 = ad.Variable(name = 'x1')\n",
    "x2 = ad.Variable(name = 'x2')\n",
    "f1 = x1 * x2 + x_1\n",
    "f2 = x1 / x2\n",
    "```\n",
    "Then when they want to evaluate the gradients of f with respect to x1 and x2, they will do\n",
    "```python\n",
    "print(f1.val, f2.val, f1.der, f2.der)\n",
    "```\n",
    "The Jacobian $\\mathbf{J}(\\mathbf{f})$ =(f1', f2') = (f1.der, f2.der)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import AutoDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define initial variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the values and derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print the values and derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "*Describe (briefly) the mathematical background and concepts as you see fit.  You **do not** need to\n",
    "give a treatise on automatic differentiation or dual numbers.  Just give the essential ideas (e.g.\n",
    "the chain rule, the graph structure of calculations, elementary functions, etc).*\n",
    "\n",
    "\n",
    "\n",
    "#### What is AD?\n",
    "\n",
    "AD is a set of techniques to numerically evaluate the derivative of a function specified by a computer program based on the fact that every computer program execute a sequence of elementary arithmetic operations and elementary functions. Using the chain rule, the derivative of each sub-expression can be calculated recursively to obtain the final derivatives. Depending on the sequence of calculating those sub-expressions, there are two major method of doing AD: **forward accumulation** and **reverse accumulation**. \n",
    "\n",
    "#### Why AD?\n",
    "\n",
    "Traditionally, there are two ways of doing differentiation, i.e., symbolic differentiation (SD) and numerical differentiation (ND). SD gives exact expression of the derivatives and produce differentiation up to machine precision, while SD is very inefficient since the expression could become very during differentiation. ND on the other hand, suffers from round-off errors (or truncate error), which leads to bad precision. Moreover, both ND and SD have problems with calculating higher derivatives and they are slow for vector inputs with large size. AD solves all of these problems nicely.\n",
    "\n",
    "#### How to do AD?\n",
    "\n",
    "Considering a simple function:\n",
    "$$z = \\cos(x)\\sin(y) + \\frac{x}{y}$$\n",
    "In AD, its computational graph for forward accumulation method looks like:\n",
    "<img src=\"figs/Fig1.png\" width=\"400\">\n",
    "Accoring to the graph, the simple function can be rewritten as\n",
    "\n",
    "\\begin{align}\n",
    "z = \\cos(x)\\sin(y) + \\frac{x}{y}=\\cos(w_1) \\sin(w_2) + \\frac{w_1}{w_2}=w_3 w_4+w_6=w_5 + w_6=w_7\n",
    "\\end{align}\n",
    "The derivates with respect to $x$ and $y$ can be calcualted according to chain rule as:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial z}{\\partial x}&=\\frac{\\partial z}{\\partial w_7}\\left(\\frac{\\partial w_7}{\\partial w_5}\\frac{\\partial w_5}{\\partial w_3}\\frac{\\partial w_3}{\\partial w_1}+\\frac{\\partial w_7}{\\partial w_6}\\frac{\\partial w_6}{\\partial w_1}\\right)\\frac{\\partial w_1}{\\partial x}\\\\\n",
    "\\frac{\\partial z}{\\partial y}&=\\frac{\\partial z}{\\partial w_7}\\left(\\frac{\\partial w_7}{\\partial w_5}\\frac{\\partial w_5}{\\partial w_4}\\frac{\\partial w_4}{\\partial w_2}+\\frac{\\partial w_7}{\\partial w_6}\\frac{\\partial w_6}{\\partial w_2}\\right)\\frac{\\partial w_2}{\\partial y}\\\\\n",
    "\\end{align}\n",
    "\n",
    "Therefore $\\frac{\\partial z}{\\partial x}$ and $\\frac{\\partial z}{\\partial y}$ are just the combinations of derivatives of elementary functions, which can be calculated analytically. In forward accumulation, the chain rule are applied from inside to outside. Computationally, the values of $w_i$ and their derivatives are store along the chain accumulatively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software Organization\n",
    "\n",
    "#### Directory structure \n",
    "```\n",
    "/cs207-FinalProject\n",
    "    /docs\n",
    "        milestone1.ipynb\n",
    "        milestone2.ipynb\n",
    "    /AutoDiff\n",
    "        __init__.py\n",
    "        AutoDiff.py\n",
    "    /tests\n",
    "        __init__.py\n",
    "        test_operator.py\n",
    "    README.md\n",
    "    requirements.txt\n",
    "    LICENSE.md\n",
    "```\n",
    "#### Modules\n",
    "\n",
    "- `__init__.py`:  initialize the package by importing necessary functions from other modules\n",
    "\n",
    "- `AutoDiff.py`:  main module of the package which implements basic data structure and algorithms of the forward automatic differentiation, including overloaded operators and special functions such as sin and trig.\n",
    "\n",
    "#### Test\n",
    "\n",
    "The test suite will live on a test_operator.py file in tests folder. We automate our testing using continuous integration. Every time we commit and push to GitHub, our code is automatically tested by `Travis CI` and `Coveralls` for code coverage. \n",
    "\n",
    "#### Package Installation\n",
    "\n",
    "Eventually we will use PyPI to distribute our package. At this point, the user needs to download and manually install the package as following.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "### Current Implementation\n",
    "#### Data structures\n",
    "*What are the core data structures?*\n",
    "\n",
    "* dictionary: we use dictionaries to keep track of the partial derivatives. The keys are the variables we differentiate with respect to and the values are the actual derivatives.\n",
    "* overloaded operators such as \\__add\\__ and \\__mul\\__ to add or multiply two auto-differentiation objects.\n",
    "\n",
    "#### Classes\n",
    "*What are the core classes?*\n",
    "\n",
    "* class Variable - an auto-differentiation class with the overloaded operators \n",
    "    * attributes\n",
    "        * val: scalar value of current node\n",
    "        * name: name of variable\n",
    "        * der: dict of partial derivatives of current node\n",
    "    * Methods\n",
    "        * \\__pos\\__\n",
    "        * \\__neg\\__\n",
    "        * \\__add\\__\n",
    "        * \\__radd\\__\n",
    "        * \\__sub\\__\n",
    "        * \\__rsub\\__\n",
    "        * \\__mul\\__\n",
    "        * \\__rmul\\__\n",
    "        * \\__itruediv\\__\n",
    "        * \\__rtruediv\\__\n",
    "        * \\__pow\\__\n",
    "        * \\__rpow\\__\n",
    "\n",
    "\n",
    "* method exp()\n",
    "    * input \n",
    "        * Variable object\n",
    "    * output \n",
    "        * Variable object after taking exponential\n",
    "\n",
    "\n",
    "* method log()\n",
    "    * input \n",
    "        * Variable object\n",
    "    * output \n",
    "        * Variable object after taking log\n",
    "\n",
    "\n",
    "* method sin()\n",
    "    * input \n",
    "        * Variable object\n",
    "    * output \n",
    "        * Variable object after taking sine\n",
    "\n",
    "\n",
    "* method cos()\n",
    "    * input \n",
    "        * Variable\n",
    "    * output \n",
    "        * Variable object after taking cosine\n",
    "\n",
    "\n",
    "* method tan()\n",
    "    * input \n",
    "        * Variable\n",
    "    * output \n",
    "        * Variable object after taking tangent\n",
    "        \n",
    "* method sinh()\n",
    "    * input \n",
    "        * Variable\n",
    "    * output \n",
    "        * Variable object after taking sinh\n",
    "      \n",
    "      \n",
    "* method cosh()\n",
    "    * input \n",
    "        * Variable\n",
    "    * output \n",
    "        * Variable object after taking cosh\n",
    "\n",
    "\n",
    "* method tanh()\n",
    "    * input \n",
    "        * Variable\n",
    "    * output \n",
    "        * Variable object after taking tanh\n",
    "        \n",
    "        \n",
    "* method arcsin()\n",
    "    * input \n",
    "        * Variable\n",
    "    * output \n",
    "        * Variable object after taking arcsin\n",
    "\n",
    "\n",
    "* method arccos()\n",
    "    * input \n",
    "        * Variable\n",
    "    * output \n",
    "        * Variable object after taking arccos\n",
    "\n",
    "\n",
    "* method arctan()\n",
    "    * input \n",
    "        * Variable\n",
    "    * output \n",
    "        * Variable object after taking arctan\n",
    "\n",
    "#### External dependecies\n",
    "\n",
    "* NumPy\n",
    "* Math\n",
    "\n",
    "#### Elementary functions\n",
    "\n",
    "Our elementary functions include the following: \n",
    "* exp\n",
    "* log\n",
    "* sin\n",
    "* cos\n",
    "* tan\n",
    "* sinh\n",
    "* cosh\n",
    "* tanh\n",
    "* arcsin\n",
    "* arccos\n",
    "* arctan\n",
    "\n",
    "### Future Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
