{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation\n",
    "\n",
    "**Due: Thursday, Nov 8th at 11:59 PM**\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Automatic differentiation (AD) is implemented in this software. More specifically, it can automatically differentiate of a python function up to machine precision and it can take derivatives of derivatives. Obtaining derivatives accurately is important because it is the key part of gradient-based optimization, which is the foundation of many machine learning algorithms. As a matter of fact, most machine learning problems can be divided into following steps:\n",
    "\n",
    "1. define a function connecting some input $X$ with some output $Y$ with a set of parameters $\\beta$ as $Y = f(X,\\beta)$;\n",
    "2. define a loss function to check how good the model is $L(X,Y,\\beta)$;\n",
    "3. find the parameter set $\\beta$ that minimize the loss function: argmin$_\\beta L(X,Y,\\beta)$.\n",
    "\n",
    "Generally the power or performance of a machine learning algorithm is limited by the third step which is can be handled by gradient-based optimization. Therefore, our software can be used in various machine learning packages and boost their performances.\n",
    "\n",
    "Furthermore, AD can also be applied to solve differential equations in various physical systems. Such as, diffusion equations, wave equations, Navierâ€“Stokes equations and other non-linear equations which cannot be solved analytically. Traditional numerical method using difference method possess error much larger than machine error. Therefore, applying AD will possibly increase the accuracy of the solvers of those differential equations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "*Describe (briefly) the mathematical background and concepts as you see fit.  You **do not** need to\n",
    "give a treatise on automatic differentiation or dual numbers.  Just give the essential ideas (e.g.\n",
    "the chain rule, the graph structure of calculations, elementary functions, etc).*\n",
    "\n",
    "\n",
    "\n",
    "#### What is AD?\n",
    "\n",
    "AD is a set of techniques to numerically evaluate the derivative of a function specified by a computer program based on the fact that every computer program execute a sequence of elementary arithmetic operations and elementary functions. Using the chain rule, the derivative of each sub-expression can be calculated recursively to obtain the final derivatives. Depending on the sequence of calculating those sub-expressions, there are two major method of doing AD: **forward accumulation** and **reverse accumulation**. \n",
    "\n",
    "#### Why AD?\n",
    "\n",
    "Traditionally, there are two ways of doing differentiation, i.e., symbolic differentiation (SD) and numerical differentiation (ND). SD gives exact expression of the derivatives and produce differentiation up to machine precision, while SD is very inefficient since the expression could become very during differentiation. ND on the other hand, suffers from round-off errors (or truncate error), which leads to bad precision. Moreover, both ND and SD have problems with calculating higher derivatives and they are slow for vector inputs with large size. AD solves all of these problems nicely.\n",
    "\n",
    "#### How to do AD?\n",
    "\n",
    "Considering a simple function:\n",
    "$$z = \\cos(x)\\sin(y) + \\frac{x}{y}$$\n",
    "In AD, its computational graph for forward accumulation method looks like:\n",
    "<img src=\"figs/Fig1.png\" width=\"400\">\n",
    "Accoring to the graph, the simple function can be rewritten as\n",
    "\n",
    "\\begin{align}\n",
    "z = \\cos(x)\\sin(y) + \\frac{x}{y}=\\cos(w_1) \\sin(w_2) + \\frac{w_1}{w_2}=w_3 w_4+w_6=w_5 + w_6=w_7\n",
    "\\end{align}\n",
    "The derivates with respect to $x$ and $y$ can be calcualted according to chain rule as:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial z}{\\partial x}&=\\frac{\\partial z}{\\partial w_7}\\left(\\frac{\\partial w_7}{\\partial w_5}\\frac{\\partial w_5}{\\partial w_3}\\frac{\\partial w_3}{\\partial w_1}+\\frac{\\partial w_7}{\\partial w_6}\\frac{\\partial w_6}{\\partial w_1}\\right)\\frac{\\partial w_1}{\\partial x}\\\\\n",
    "\\frac{\\partial z}{\\partial y}&=\\frac{\\partial z}{\\partial w_7}\\left(\\frac{\\partial w_7}{\\partial w_5}\\frac{\\partial w_5}{\\partial w_4}\\frac{\\partial w_4}{\\partial w_2}+\\frac{\\partial w_7}{\\partial w_6}\\frac{\\partial w_6}{\\partial w_2}\\right)\\frac{\\partial w_2}{\\partial y}\\\\\n",
    "\\end{align}\n",
    "\n",
    "Therefore $\\frac{\\partial z}{\\partial x}$ and $\\frac{\\partial z}{\\partial y}$ are just the combinations of derivatives of elementary functions, which can be calculated analytically. In forward accumulation, the chain rule are applied from inside to outside. Computationally, the values of $w_i$ and their derivatives are store along the chain accumulatively.\n",
    "\n",
    "### Operator overloading\n",
    "\n",
    "Operator overloading is applied in this package to realize AD. Values and derivatives of functions are updated and passed at each operation. That means if we have two functions $\\omega_1 (x_1,x_2,...,x_n)$ and $\\omega_2 (x_1,x_2,...,x_n)$ with their derivatives ${\\partial\\omega_1}/{\\partial x_i}$ and ${\\partial\\omega_1}/{\\partial x_i}$ and second derivatives ${\\partial^2\\omega_1}/{\\partial x_i}{\\partial x_j}$ and ${\\partial^2\\omega_2}/{\\partial x_i}{\\partial x_j}$, $\\forall i,j$. For a new variable $\\omega_3 = \\omega_1*\\omega_2$, where $*$ is some operator (addition, multiplication, division, power, etc). Accoring to specific types of the operator we can write down analytical expression of the values as well the derivatives of $\\omega_3$ as functions of $\\omega_1$ and $\\omega_2$ and their derivatives. The following expression shows these analytical expressions:\n",
    "\n",
    "| operator \t| &nbsp; &nbsp; &nbsp;value $\\omega_3$&nbsp; | &nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; first derivative $\\frac{\\partial \\omega_3}{\\partial x_i}$  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp; &nbsp;| second derivative $\\frac{\\partial^2 \\omega_3}{\\partial x_i \\partial x_j }$                                                                  \t|   \t|\n",
    "|:---:|:---:|:---:|:---:|---\t|\n",
    "|    $+$   \t|         $\\omega_1 + \\omega_2$         \t|$\\frac{\\partial \\omega_1}{\\partial x_i} +\\frac{\\partial \\omega_2}{\\partial x_i}$                                    \t|                                                                                                                                                                                                                                                                                     $\\frac{\\partial^2 \\omega_1}{\\partial x_i \\partial x_j } +\\frac{\\partial^2 \\omega_2}{\\partial x_i \\partial x_j }$                                                                                                                                                                                                                                                                                    \t|   \t|\n",
    "|    $-$   \t|    $\\omega_1 - \\omega_2$   \t|                                    $\\frac{\\partial \\omega_1}{\\partial x_i} - \\frac{\\partial \\omega_2}{\\partial x_i}$                                   \t|                                                                                                                                                                                                                                                                                     $ \\frac{\\partial^2 \\omega_1}{\\partial x_i \\partial x_j } -\\frac{\\partial^2 \\omega_2}{\\partial x_i \\partial x_j }$                                                                                                                                                                                                                                                                                    \t|   \t|\n",
    "|    $\\times$   \t| $\\omega_1 \\times \\omega_2$ \t|                           $ \\omega_2 \\frac{\\partial \\omega_1}{\\partial x_i} +\\omega_1\\frac{\\partial \\omega_2}{\\partial x_i}$                           \t|                                                                                                                                                                                             $ \\frac{\\partial \\omega_1}{\\partial x_i}\\frac{\\partial \\omega_2}{\\partial x_j} + \\frac{\\partial \\omega_1}{\\partial x_j}\\frac{\\partial \\omega_2}{\\partial x_i}+ \\omega_2 \\frac{\\partial^2 \\omega_1}{\\partial x_i \\partial x_j } + \\omega_1 \\frac{\\partial^2 \\omega_2}{\\partial x_i \\partial x_j }$                                                                                                                                                                                            \t|   \t|\n",
    "|    $/$   \t|    $\\omega_1 / \\omega_2$   \t|            $\\frac{1}{\\omega_2} \\frac{\\partial \\omega_1}{\\partial x_i} - \\frac{\\omega_1}{\\omega_2^2} \\frac{\\partial \\omega_2}{\\partial x_i}$            \t|                                                                                                        $ -\\frac{1}{\\omega_2^2}\\left(\\frac{\\partial \\omega_1}{\\partial x_i}\\frac{\\partial \\omega_2}{\\partial x_j} + \\frac{\\partial \\omega_1}{\\partial x_j}\\frac{\\partial \\omega_2}{\\partial x_i}\\right) + \\frac{2\\omega_1}{\\omega_2^3}\\frac{\\partial \\omega_2}{\\partial x_i}\\frac{\\partial \\omega_2}{\\partial x_j}+\\frac{1}{\\omega_2} \\frac{\\partial^2 \\omega_1}{\\partial x_i \\partial x_j } - \\frac{\\omega_1}{\\omega_2^2} \\frac{\\partial^2 \\omega_2}{\\partial x_i \\partial x_j }$                                                                                                        \t|   \t|\n",
    "| ^     | $\\omega_1^{\\omega_2}$      \t| $\\omega_1 ^{\\omega_2-1} \\omega_2\\frac{\\partial \\omega_1}{\\partial x_i}+\\ln(\\omega_1) \\omega_1^{\\omega_2} \\frac{\\partial \\omega_2}{\\partial x_i}$ \t| $\\omega_1^{\\omega_2-2}(\\omega_2^2 - \\omega_2) \\frac{\\partial \\omega_1}{\\partial x_i}\\frac{\\partial \\omega_1}{\\partial x_j}+ (\\omega_1^{\\omega_2-1}+\\omega_2\\ln(\\omega_1)\\omega_1^{\\omega_2-1})\\left( \\frac{\\partial \\omega_1}{\\partial x_i}\\frac{\\partial \\omega_2}{\\partial x_j} +\\frac{\\partial \\omega_1}{\\partial x_j}\\frac{\\partial \\omega_2}{\\partial x_i}\\right)+\\ln (\\omega_1)^2\\omega_1^{\\omega_2}\\frac{\\partial \\omega_2}{\\partial x_i}\\frac{\\partial \\omega_2}{\\partial x_j} + \\omega_2 \\omega_1 ^{\\omega_2-1}\\frac{\\partial^2 \\omega_1}{\\partial x_i \\partial x_j } + \\ln(\\omega_1) \\omega_1^{\\omega_2}\\frac{\\partial^2 \\omega_2}{\\partial x_i \\partial x_j }$ \t|   \t|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use *AutoDiff*\n",
    "\n",
    "There are two ways to install our package.\n",
    "\n",
    "### Method 1: User installation via ```pip```\n",
    "Our package is currently published on PyPI, as ```AutoDiff-XVQD```. With the forward mode implementation complete, users can currently install our package via ```pip```. After creating a new directory for their project, the user should go into this directory and enter in the following commands:\n",
    "\n",
    "Within the directory the user just created, they create a virtual environment and call it `env`.\n",
    "```bash\n",
    "virtualenv env\n",
    "```\n",
    "\n",
    "Then they activate the virtual environment and install the package.\n",
    "```bash\n",
    "source env/bin/activate\n",
    "pip install AutoDiff-XVQD\n",
    "```\n",
    "\n",
    "After these steps, the user can open a Python interpreter on the virtual environment they just created and now they will be able to import the module.\n",
    "\n",
    "```python\n",
    ">>> import AutoDiff.AutoDiff as ad\n",
    "```\n",
    "\n",
    "### Method 2: Installation via Github (for developers and users)\n",
    "Since our package is not on PyPI yet, the user needs to create a virtual environment and manually install the package. First, the user needs to download the package from GitHub. Then they create a directory and unpack the project into that directory. \n",
    "```bash\n",
    "git clone https://github.com/XVQD/cs207-FinalProject.git\n",
    "```\n",
    "Within the directory the user just created, they create a virtual environment and call it `env`.\n",
    "```bash\n",
    "virtualenv env\n",
    "```\n",
    "\n",
    "Then they activate the virtual environment and install the dependencies.\n",
    "```bash\n",
    "source env/bin/activate\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "After these steps, the user can open a Python interpreter on the virtual environment they just created and now they will be able to import the module.\n",
    "\n",
    "```python\n",
    ">>> import AutoDiff.AutoDiff as ad\n",
    "```\n",
    "\n",
    "### Introduction to basic usage of the package\n",
    "\n",
    "After successful installation, the user will first import our package.\n",
    "```python\n",
    ">>> import AutoDiff.AutoDiff as ad\n",
    "```\n",
    "Then depending on the type of expressions they have, they will employ one of the following methods.\n",
    "\n",
    "#### Scalar functions of scalar values\n",
    "Say the user wants to get the gradient of the expression $f(x) = alpha * x + 3$.\n",
    "The user will first create a variable x and then define the symbolic expression for `f`.\n",
    "```python\n",
    ">>> x = ad.Variable(2, name='x')\n",
    ">>> f = 2 * x + 3\n",
    "```\n",
    "Note: If the user wants to include special functions like sin and exp, they need to do the following:\n",
    "```python\n",
    ">>> f = 2 * ad.sin(x) + 3\n",
    "```\n",
    "Then when they want to evaluate the gradients of f with respect to x, they will do\n",
    "```python\n",
    ">>> print(f.val, f.der)\n",
    "```\n",
    "f.val and f.der will then contain the value and gradient of f with respect to x.\n",
    "\n",
    "#### Scalar functions of vectors\n",
    "Say the user wants to get the gradient of the expression $f(x1,x2) = x_1 x_2 + x_1$. \n",
    "\n",
    "The user will first create two variables `x1` and `x2` and then define the symbolic expression for `f`.\n",
    "```python\n",
    ">>> x1 = ad.Variable(2,name='x1')\n",
    ">>> x2 = ad.Variable(3,name='x2')\n",
    ">>> f = x1 * x2 + x_1\n",
    "```\n",
    "Then when they want to get the values and gradients of f with respect to x1 and x2, they will do\n",
    "```python\n",
    ">>> print(f.val, f.der)\n",
    "```\n",
    "f.val and f.der will then contain dictionaries of values and gradients of f with respect to x1 and x2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector functions of vectors\n",
    "Say the user wants to get the gradients of the system of functions \n",
    "$$f_1 = x_1 x_2 + x_1$$\n",
    "$$f_2 = \\frac{x_1}{x_2}$$\n",
    "\n",
    "i.e.\n",
    "$$\\mathbf{f}(x1,x2)=(f_1(x_1,x_2),f_2(x_1,x_2))$$\n",
    "The user will first create two variables `x1` and `x2` and then define the symbolic expression for `f`.\n",
    "```python\n",
    ">>> x1 = ad.Variable(3, name = 'x1')\n",
    ">>> x2 = ad.Variable(2, name = 'x2')\n",
    ">>> f1 = x1 * x2 + x_1\n",
    ">>> f2 = x1 / x2\n",
    "```\n",
    "Then when they want to evaluate the gradients of f with respect to x1 and x2, they will do\n",
    "```python\n",
    ">>> print(f1.val, f2.val, f1.der, f2.der)\n",
    "```\n",
    "The Jacobian $\\mathbf{J}(\\mathbf{f})$ =(f1', f2') = (f1.der, f2.der)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo\n",
    "\n",
    "The following demo shows a simple case of using the AutoDiff package to calculate the derivatives. This can be done in the directory the user creates AFTER they install the package as shown in 'installation' section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First the user imports the package `AutoDiff`\n",
    "```python\n",
    ">>> import AutoDiff.AutoDiff as ad\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Then they define the variables. For example, if they have variables $x_1$ and $x_2$, which they want to evaluate at 3 and 2 respectively, they will define them as the following. The user needs to give a name as a parameter when they define the variables.\n",
    "```python\n",
    ">>> x1 = ad.Variable(3, name='x1')\n",
    ">>> x2 = ad.Variable(2, name='x2')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Then the user writes down the expression/function they want to evaluate. Most of the expression would be normal arithmetic expressions, except for the special functions such as exp, sin, cos, etc., the user needs to use ad.exp(), ad.sin(), ad.cos(), etc.\n",
    "```python\n",
    ">>> f = ad.sin(x1) + x2**2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Finally, the user can get print the value and derivatives of their expression with respect to $x_1$ and $x_2$. This will output the value as a scalar and the partial derivatives as a dictionary where the keys are the variable names and the values are the derivatives with respect to that variable.\n",
    "```python\n",
    ">>> print(f.val, f.der)\n",
    "4.141120008059867 {'x2': 4, 'x1': -0.9899924966004454}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software Organization\n",
    "\n",
    "#### Directory structure \n",
    "```\n",
    "/cs207-FinalProject\n",
    "    /docs\n",
    "        milestone1.ipynb\n",
    "        milestone2.ipynb\n",
    "    /AutoDiff\n",
    "        __init__.py\n",
    "        AutoDiff.py\n",
    "    /tests\n",
    "        __init__.py\n",
    "        test_operator.py\n",
    "        test_elementary_functions.py\n",
    "    README.md\n",
    "    requirements.txt\n",
    "    LICENSE.md\n",
    "```\n",
    "#### Modules\n",
    "\n",
    "- `__init__.py`:  initialize the package by importing necessary functions from other modules\n",
    "\n",
    "- `AutoDiff.py`:  main module of the package which implements basic data structure and algorithms of the forward automatic differentiation, including overloaded operators and special functions such as sin and trig.\n",
    "\n",
    "#### Test\n",
    "\n",
    "The test suite will live on `test_operator.py` and `test_elementary_functions.py` files in tests folder. `test_operator.py` tests the overloaded operators and `test_elementary_functions.py` tests the elementary functions such as exp, cos, sin, etc. We automate our testing using continuous integration. Every time we commit and push to GitHub, our code is automatically tested by `Travis CI` and `Coveralls` for code coverage. \n",
    "\n",
    "#### Package Installation\n",
    "\n",
    "Eventually we will use PyPI to distribute our package. At this point, the user needs to download and manually install the package as following.\n",
    "\n",
    "First, the user needs to download the package from GitHub. Then they create a directory and unpack the project into that directory.\n",
    "```bash\n",
    "git clone https://github.com/XVQD/cs207-FinalProject.git\n",
    "```\n",
    "\n",
    "Within the directory the user just created, they create a virtual environment and call it env.\n",
    "```bash\n",
    "cd yourdir\n",
    "virtualenv env\n",
    "```\n",
    "\n",
    "Then they activate the virtual environment and install the dependencies.\n",
    "```bash\n",
    "source env/bin/activate\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "After these steps, the user can open a Python interpreter on the virtual environment they just created and now they will be able to import the package.\n",
    "```python\n",
    ">>> import AutoDiff.AutoDiff as ad\n",
    "```\n",
    "\n",
    "## Implementation\n",
    "\n",
    "### Current Implementation\n",
    "#### Data structures\n",
    "*What are the core data structures?*\n",
    "\n",
    "* dictionary: we use dictionaries to keep track of the partial derivatives. The keys are the variables we differentiate with respect to and the values are the actual derivatives.\n",
    "* overloaded operators such as \\__add\\__ and \\__mul\\__ to add or multiply two auto-differentiation objects.\n",
    "\n",
    "#### Classes\n",
    "*What are the core classes?*\n",
    "\n",
    "* class Variable - an auto-differentiation class with the overloaded operators \n",
    "    * attributes\n",
    "        * val: scalar value of current node\n",
    "        * name: name of variable\n",
    "        * der: dict of partial derivatives of current node\n",
    "    * Methods\n",
    "        * \\__pos\\__\n",
    "        * \\__neg\\__\n",
    "        * \\__add\\__\n",
    "        * \\__radd\\__\n",
    "        * \\__sub\\__\n",
    "        * \\__rsub\\__\n",
    "        * \\__mul\\__\n",
    "        * \\__rmul\\__\n",
    "        * \\__itruediv\\__\n",
    "        * \\__rtruediv\\__\n",
    "        * \\__pow\\__\n",
    "        * \\__rpow\\__\n",
    "\n",
    "\n",
    "* method exp()\n",
    "    * input \n",
    "        * Variable object\n",
    "    * output \n",
    "        * Variable object after taking exponential\n",
    "\n",
    "\n",
    "* method log()\n",
    "    * input \n",
    "        * Variable object\n",
    "    * output \n",
    "        * Variable object after taking log\n",
    "\n",
    "\n",
    "* method sin()\n",
    "    * input \n",
    "        * Variable object\n",
    "    * output \n",
    "        * Variable object after taking sine\n",
    "\n",
    "\n",
    "* method cos()\n",
    "    * input \n",
    "        * Variable\n",
    "    * output \n",
    "        * Variable object after taking cosine\n",
    "\n",
    "\n",
    "* method tan()\n",
    "    * input \n",
    "        * Variable\n",
    "    * output \n",
    "        * Variable object after taking tangent\n",
    "        \n",
    "* method sinh()\n",
    "    * input \n",
    "        * Variable\n",
    "    * output \n",
    "        * Variable object after taking sinh\n",
    "      \n",
    "      \n",
    "* method cosh()\n",
    "    * input \n",
    "        * Variable\n",
    "    * output \n",
    "        * Variable object after taking cosh\n",
    "\n",
    "\n",
    "* method tanh()\n",
    "    * input \n",
    "        * Variable\n",
    "    * output \n",
    "        * Variable object after taking tanh\n",
    "        \n",
    "        \n",
    "* method arcsin()\n",
    "    * input \n",
    "        * Variable\n",
    "    * output \n",
    "        * Variable object after taking arcsin\n",
    "\n",
    "\n",
    "* method arccos()\n",
    "    * input \n",
    "        * Variable\n",
    "    * output \n",
    "        * Variable object after taking arccos\n",
    "\n",
    "\n",
    "* method arctan()\n",
    "    * input \n",
    "        * Variable\n",
    "    * output \n",
    "        * Variable object after taking arctan\n",
    "\n",
    "#### External dependecies\n",
    "\n",
    "* numpy==1.15.4\n",
    "* scipy==1.1.0\n",
    "\n",
    "#### Elementary functions\n",
    "\n",
    "Our elementary functions include the following: \n",
    "* exp\n",
    "* log\n",
    "* sin\n",
    "* cos\n",
    "* tan\n",
    "* sinh\n",
    "* cosh\n",
    "* tanh\n",
    "* arcsin\n",
    "* arccos\n",
    "* arctan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Future\n",
    "*Now that you've got most of the hard implementation work done, what kinds of things do you want to impelement next? How will your software change? What will be the primary challenges?*\n",
    "\n",
    "### Higher derivatives\n",
    "At this point we stored the first derivative (gradient) of the function in a vector `f.der`. Principally we can also keep track of higher derivatives in vectors `f.der2`, `f.der3`, `f.der4`, etc. The specific implementation the higher derivatives will not change the exsiting code structure. The primary challenge would be define the overloaded operators and elementary functions for higher derivatives.\n",
    "\n",
    "### Newton solver\n",
    "In engineering as well as science, we always need to solve sets of nonlinear equations, i.e., find the solution of \n",
    "\n",
    "$$\\mathbf{F}(\\mathbf{X})=\\mathbf{0}$$\n",
    "\n",
    "where $\\mathbf{X}$ and $\\mathbf{F}$ are high dimensional vectors. Typically Newton's method is used to iteratively solve this equation,\n",
    "\n",
    "$$\\mathbf{X_{n+1}} = \\mathbf{X_{n}} - \\left(\\mathbf{F'}(\\mathbf{X_n})\\right)^{-1} \\mathbf{F}(\\mathbf{X_n})$$\n",
    "\n",
    "where $\\mathbf{F'}(\\mathbf{X_n}$ is the Jacobian of $\\mathbf{F}$. The iteration ends when $\\|\\mathbf{F}(\\mathbf{X_n})\\|$ is sufficiently closed to zero.\n",
    "\n",
    "The accuracy and robustness of Newton's method highly depend on the tolerence of the calculated Jocobian, which can be calculated using our AD package. As for implementation, we define function $\\mathbf{F_0}$ with some initial guess of solution $\\mathbf{X_0}$, then use Newton's method to find $\\mathbf{X_1}$, then update the function to $\\mathbf{F_1}$, then move on and on until $\\mathbf{F_n}(\\mathbf{X_n})$ is sufficiently closed to zero. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
